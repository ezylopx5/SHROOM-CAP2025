#!/usr/bin/env python3
"""
Pre-flight validation script for SHROOM-CAP pipeline
Tests critical components before Lightning AI deployment
"""

import os
import sys
import json
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

def validate_data_structure():
    """Validate SHROOM data structure"""
    logger.info("üîç Validating SHROOM data structure...")
    
    data_dir = Path("SHROOM_DATA")
    if not data_dir.exists():
        logger.error("‚ùå SHROOM_DATA directory not found!")
        return False
    
    # Check training data
    train_dir = data_dir / "TrainSet V1"
    if not train_dir.exists():
        logger.error("‚ùå TrainSet V1 directory not found!")
        return False
    
    # Check for data files
    data_files = list(train_dir.glob("*_train_data.jsonl"))
    label_files = list(train_dir.glob("*_train_label.jsonl"))
    
    logger.info(f"‚úÖ Found {len(data_files)} data files")
    logger.info(f"‚úÖ Found {len(label_files)} label files")
    
    if len(data_files) == 0:
        logger.error("‚ùå No training data files found!")
        return False
    
    if len(label_files) == 0:
        logger.error("‚ùå No training label files found!")
        return False
    
    # Check test data
    test_dir = data_dir / "Test"
    if test_dir.exists():
        test_files = list(test_dir.glob("*_test_data.jsonl"))
        logger.info(f"‚úÖ Found {len(test_files)} test files")
    else:
        logger.warning("‚ö†Ô∏è  No Test directory found")
    
    # Validate file pairs
    languages = set()
    for data_file in data_files:
        lang = data_file.stem.split('_')[0]
        languages.add(lang)
        
        label_file = train_dir / f"{lang}_train_label.jsonl"
        if not label_file.exists():
            logger.error(f"‚ùå Missing label file for {lang}")
            return False
    
    logger.info(f"‚úÖ Found data for languages: {sorted(languages)}")
    
    # Test loading a sample
    try:
        sample_data_file = data_files[0]
        with open(sample_data_file, 'r') as f:
            sample_line = f.readline().strip()
            sample_data = json.loads(sample_line)
            
        logger.info("‚úÖ Sample data structure:")
        logger.info(f"   Keys: {list(sample_data.keys())}")
        
        # Check essential fields
        essential_fields = ['question', 'output_text']
        for field in essential_fields:
            if field not in sample_data:
                logger.error(f"‚ùå Missing essential field: {field}")
                return False
        
        # Test label loading
        lang = sample_data_file.stem.split('_')[0]
        label_file = train_dir / f"{lang}_train_label.jsonl"
        with open(label_file, 'r') as f:
            sample_label_line = f.readline().strip()
            sample_label = json.loads(sample_label_line)
            
        logger.info(f"‚úÖ Sample label structure: {sample_label}")
        
    except Exception as e:
        logger.error(f"‚ùå Error reading sample data: {e}")
        return False
    
    return True

def validate_scripts():
    """Validate all Python scripts compile correctly"""
    logger.info("üîç Validating Python scripts...")
    
    scripts = [
        'shroom_data_processor.py',
        'checkpoint_system.py', 
        'shroom_trainer.py',
        'lightning_ai_start.py'
    ]
    
    for script in scripts:
        if not Path(script).exists():
            logger.error(f"‚ùå Missing script: {script}")
            return False
        
        # Test compilation
        try:
            import py_compile
            py_compile.compile(script, doraise=True)
            logger.info(f"‚úÖ {script} - syntax OK")
        except py_compile.PyCompileError as e:
            logger.error(f"‚ùå {script} - syntax error: {e}")
            return False
    
    return True

def validate_imports():
    """Test key imports work"""
    logger.info("üîç Testing critical imports...")
    
    try:
        # Test data processor
        sys.path.insert(0, '.')
        from shroom_data_processor import SHROOMDataProcessor
        logger.info("‚úÖ SHROOMDataProcessor import OK")
        
        # Test checkpoint system
        from checkpoint_system import CheckpointSystem
        logger.info("‚úÖ CheckpointSystem import OK")
        
        # Test trainer (basic import)
        from shroom_trainer import SHROOMEnsembleTrainer
        logger.info("‚úÖ SHROOMEnsembleTrainer import OK")
        
        return True
        
    except ImportError as e:
        logger.error(f"‚ùå Import error: {e}")
        return False

def validate_notebook():
    """Validate Colab notebook JSON"""
    logger.info("üîç Validating Colab notebook...")
    
    notebook_file = Path("SHROOM_Colab_Migration.ipynb")
    if not notebook_file.exists():
        logger.error("‚ùå Colab notebook not found!")
        return False
    
    try:
        with open(notebook_file, 'r') as f:
            notebook_data = json.load(f)
        
        # Check basic structure
        if 'cells' not in notebook_data:
            logger.error("‚ùå Invalid notebook structure - no cells")
            return False
        
        logger.info(f"‚úÖ Notebook has {len(notebook_data['cells'])} cells")
        
        # Check for GPU setting
        metadata = notebook_data.get('metadata', {})
        if 'accelerator' in metadata and metadata['accelerator'] == 'GPU':
            logger.info("‚úÖ Notebook configured for GPU")
        else:
            logger.warning("‚ö†Ô∏è  Notebook not configured for GPU")
        
        return True
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Invalid notebook JSON: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error reading notebook: {e}")
        return False

def run_validation():
    """Run complete validation"""
    logger.info("üçÑ‚ö° SHROOM-CAP Pipeline Validation")
    logger.info("="*50)
    
    all_valid = True
    
    # Validate data structure
    if not validate_data_structure():
        all_valid = False
    
    # Validate scripts
    if not validate_scripts():
        all_valid = False
    
    # Validate imports
    if not validate_imports():
        all_valid = False
    
    # Validate notebook
    if not validate_notebook():
        all_valid = False
    
    logger.info("="*50)
    
    if all_valid:
        logger.info("üéâ ALL VALIDATIONS PASSED!")
        logger.info("‚úÖ Pipeline is ready for Lightning AI deployment")
        logger.info("")
        logger.info("üìã Next steps:")
        logger.info("1. Upload SHROOM_DATA folder to Lightning AI")
        logger.info("2. Upload all Python scripts to Lightning AI")
        logger.info("3. Select A100 GPU machine")
        logger.info("4. Run: python lightning_ai_start.py")
        logger.info("")
        logger.info("üöÄ Good luck with your training!")
        return True
    else:
        logger.error("‚ùå VALIDATION FAILED!")
        logger.error("Please fix the issues above before deploying")
        return False

if __name__ == "__main__":
    success = run_validation()
    sys.exit(0 if success else 1)