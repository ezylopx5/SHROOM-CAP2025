ðŸŽ¯ About This Competition
SHROOM-CAP 2025 is a shared task focused on detecting hallucinations in multilingual scientific text generated by Large Language Models (LLMs). The competition evaluates systems across 9 languages (5 training + 4 zero-shot) for binary classification of factual and fluency mistakes in scientific domains.

Task: Detect if LLM-generated scientific text contains hallucinations (binary classification)
Languages: English, Spanish, French, Hindi, Italian (training) + Bengali, Gujarati, Malayalam, Telugu (zero-shot)
Evaluation: Macro F1 score for Factuality and Fluency mistakes

ðŸš€ Quick Setup & Execution
Follow these steps to reproduce our results:

Step 1: Install Dependencies

# Clone the repository
git clone https://github.com/ezylopx5/SHROOM2025.git
cd SHROOM2025

# Install required packages
pip install -r requirements_clean.txt

Step 2: Validate Setup


# Check if everything is configured correctly
python validate_pipeline.py
This script will verify:

All required packages are installed

Data files are in correct locations

GPU availability (if applicable)

Model dependencies

Step 3: Training (Two Options Available)
We trained our model on an NVIDIA H200 GPU (141GB VRAM). Choose one of these approaches:

Option A: Use the Guided Training Script (Recommended)

# Interactive training with progress monitoring
python start_training_full_dataset.py
This provides:

Progress bars and estimated completion time

Automatic checkpoint saving every 2-3 hours

Resume capability if interrupted

Expected: 2-3 days training time

Option B: Direct Training Command

# Run training directly
python train_xlm.py \
    --model_name xlm-roberta-large \
    --data_dir SHROOM_DATA \
    --output_dir outputs_xlm_large_full_dataset \
    --epochs 3 \
    --batch_size 16 \
    --learning_rate 2e-5 \
    --max_length 256
    
Step 4: Generate Competition Submissions
After training completes, generate predictions for all 9 languages:

python generate_final_submission.py \
    --model_path outputs_xlm_large_full_dataset/fold_0/best_model \
    --data_dir SHROOM_DATA \
    --output_dir submissions
This creates JSONL files for each language in the submissions/ directory, ready for upload to the competition platform.

ðŸ’¡ Key Features of Our Approach
Data-Centric Strategy: Unified 5 datasets â†’ 124,821 balanced samples (172x increase)

Multilingual Power: XLM-RoBERTa-Large fine-tuned on 100 languages

Infrastructure: Optimized for H200 GPU (1h14m training time)

ðŸ“Š Project Structure

SHROOM2025/
â”œâ”€â”€ SHROOM_DATA/                 # Our unified 124k dataset
â”œâ”€â”€ generate_final_submission.py # Step 4: Create submissions
â”œâ”€â”€ start_training_full_dataset.py # Step 3A: Guided training
â”œâ”€â”€ train_xlm.py                # Step 3B: Direct training
â”œâ”€â”€ validate_pipeline.py        # Step 2: Setup validation
â”œâ”€â”€ requirements_clean.txt      # Step 1: Dependencies
â””â”€â”€ TRAINING_STRATEGY_GUIDE.md  # Detailed methodology


Competition Leaderboard: https://shroomcap.pythonanywhere.com/

